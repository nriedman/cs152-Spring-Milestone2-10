{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Type of Source</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Geographical_Location</th>\n",
       "      <th>Author_Country_Affiliation</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chatfield et.al \"Tweeting propaganda, radicali...</td>\n",
       "      <td>Research Article</td>\n",
       "      <td>Coalition planes massacred these children in a...</td>\n",
       "      <td>Propaganda</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chatfield et.al \"Tweeting propaganda, radicali...</td>\n",
       "      <td>Research Article</td>\n",
       "      <td>these PKK fellas are exceptional liars.after t...</td>\n",
       "      <td>Propaganda</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chatfield et.al \"Tweeting propaganda, radicali...</td>\n",
       "      <td>Research Article</td>\n",
       "      <td>This is so awesome. US airstrikes also by mist...</td>\n",
       "      <td>Propaganda</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chatfield et.al \"Tweeting propaganda, radicali...</td>\n",
       "      <td>Research Article</td>\n",
       "      <td>RT @ImtiyazAzhar: Support &amp;amp; love for #Isla...</td>\n",
       "      <td>Propaganda</td>\n",
       "      <td>India</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chatfield et.al \"Tweeting propaganda, radicali...</td>\n",
       "      <td>Research Article</td>\n",
       "      <td>Ask the Americans how they liked fighting\\nJTJ...</td>\n",
       "      <td>Propaganda</td>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Source    Type of Source  \\\n",
       "0  Chatfield et.al \"Tweeting propaganda, radicali...  Research Article   \n",
       "1  Chatfield et.al \"Tweeting propaganda, radicali...  Research Article   \n",
       "2  Chatfield et.al \"Tweeting propaganda, radicali...  Research Article   \n",
       "3  Chatfield et.al \"Tweeting propaganda, radicali...  Research Article   \n",
       "4  Chatfield et.al \"Tweeting propaganda, radicali...  Research Article   \n",
       "\n",
       "                                                Text       Label  \\\n",
       "0  Coalition planes massacred these children in a...  Propaganda   \n",
       "1  these PKK fellas are exceptional liars.after t...  Propaganda   \n",
       "2  This is so awesome. US airstrikes also by mist...  Propaganda   \n",
       "3  RT @ImtiyazAzhar: Support &amp; love for #Isla...  Propaganda   \n",
       "4  Ask the Americans how they liked fighting\\nJTJ...  Propaganda   \n",
       "\n",
       "  Geographical_Location Author_Country_Affiliation  Unnamed: 6  \n",
       "0                  Iraq                        USA         NaN  \n",
       "1                  Iraq                        USA         NaN  \n",
       "2                  Iraq                        USA         NaN  \n",
       "3                 India                        USA         NaN  \n",
       "4                   USA                        USA         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Type_of_Source</th>\n",
       "      <th>Text</th>\n",
       "      <th>Ideology</th>\n",
       "      <th>Label</th>\n",
       "      <th>Geographical_Location</th>\n",
       "      <th>Author_Country_Affiliation</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ray and Marsh \"Recruitment by extremist groups...</td>\n",
       "      <td>Research Article</td>\n",
       "      <td>This is a deliberate choice of words. As we st...</td>\n",
       "      <td>White Supremacist</td>\n",
       "      <td>Propaganda</td>\n",
       "      <td>-</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ray and Marsh \"Recruitment by extremist groups...</td>\n",
       "      <td>Research Article</td>\n",
       "      <td>Most victims of race crime - about 90 per cent...</td>\n",
       "      <td>White Supremacist</td>\n",
       "      <td>Propaganda</td>\n",
       "      <td>-</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ray and Marsh \"Recruitment by extremist groups...</td>\n",
       "      <td>Research Article</td>\n",
       "      <td>WE BELIEVE that the Cananite Jew is the natura...</td>\n",
       "      <td>White Supremacist</td>\n",
       "      <td>Radicalization</td>\n",
       "      <td>-</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ray and Marsh \"Recruitment by extremist groups...</td>\n",
       "      <td>Research Article</td>\n",
       "      <td>The culture of a race, free of alien influence...</td>\n",
       "      <td>White Supremacist</td>\n",
       "      <td>Radicalization</td>\n",
       "      <td>-</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ray and Marsh \"Recruitment by extremist groups...</td>\n",
       "      <td>Research Article</td>\n",
       "      <td>Influential organizations and much of the west...</td>\n",
       "      <td>White Supremacist</td>\n",
       "      <td>Propaganda</td>\n",
       "      <td>Switzerland, Germany</td>\n",
       "      <td>USA</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Source    Type_of_Source  \\\n",
       "0  Ray and Marsh \"Recruitment by extremist groups...  Research Article   \n",
       "1  Ray and Marsh \"Recruitment by extremist groups...  Research Article   \n",
       "2  Ray and Marsh \"Recruitment by extremist groups...  Research Article   \n",
       "3  Ray and Marsh \"Recruitment by extremist groups...  Research Article   \n",
       "4  Ray and Marsh \"Recruitment by extremist groups...  Research Article   \n",
       "\n",
       "                                                Text           Ideology  \\\n",
       "0  This is a deliberate choice of words. As we st...  White Supremacist   \n",
       "1  Most victims of race crime - about 90 per cent...  White Supremacist   \n",
       "2  WE BELIEVE that the Cananite Jew is the natura...  White Supremacist   \n",
       "3  The culture of a race, free of alien influence...  White Supremacist   \n",
       "4  Influential organizations and much of the west...  White Supremacist   \n",
       "\n",
       "            Label Geographical_Location Author_Country_Affiliation  Unnamed: 7  \n",
       "0      Propaganda                     -                        USA         NaN  \n",
       "1      Propaganda                     -                        USA         NaN  \n",
       "2  Radicalization                     -                        USA         NaN  \n",
       "3  Radicalization                     -                        USA         NaN  \n",
       "4      Propaganda  Switzerland, Germany                        USA         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Examine the seed datasets\n",
    "ISIS_seed = pd.read_csv('Seed_MIWS/Seed_Dataset/ISIS_Seed_Complete.csv')\n",
    "WS_seed = pd.read_csv('Seed_MIWS/Seed_Dataset/WS_Seed_Complete.csv')\n",
    "\n",
    "display(ISIS_seed.head())\n",
    "display(WS_seed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Given a path to a .csv file, this function will read the file and format it as a dicionary with the following structure:\n",
    "    \n",
    "    {\n",
    "        'Inputs': [message1, message2, ...],\n",
    "        'Labels': [label1, label2, ...]\n",
    "    }\n",
    "    \n",
    "    where 'Inputs' is a list of lists of message dictionaries that conform to the input to OpenAI's Chat Completion API,\n",
    "    and 'Labels' is a list of labels for each message. Each label is a string that represents the type of extremist content of the message.\n",
    "\n",
    "    Each message should have the following format:\n",
    "    [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'system message'\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'tweet text'\n",
    "        }\n",
    "    ]\n",
    "\"\"\"\n",
    "def format_eval_openai(path, system_message, labels_present=True, limit=None):\n",
    "    df = pd.read_csv(path, encoding='latin-1')\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        if limit and index >= limit:\n",
    "            break\n",
    "        tweet_text = row['Text']\n",
    "        inputs.append([\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': system_message\n",
    "            },\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': tweet_text\n",
    "            }\n",
    "        ])\n",
    "        if labels_present:\n",
    "            labels.append(row['Label'])\n",
    "        else:\n",
    "            labels.append(\"None\")\n",
    "    return {\n",
    "        'Inputs': inputs,\n",
    "        'Labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given a path to a .csv file, this function will read the file and format it as a dicionary with the following structure:\n",
    "        \n",
    "        {\n",
    "            'Inputs': [request1, request2, ...]),\n",
    "            'Labels': [label1, label2, ...]\n",
    "        }\n",
    "        \n",
    "        where 'Inputs' is a list of request dictionaries that conform to the input to Perspective's comments().analyze API,\n",
    "        and 'Labels' is a list of labels for each message. Each label is a string that represents the type of extremist content of the message.\n",
    "    \n",
    "        Each request will have the following format:\n",
    "        {\n",
    "            'comment': { 'text': 'tweet text' },\n",
    "            'requestedAttributes': metrics\n",
    "        }\n",
    "\n",
    "Args:\n",
    "    path: The path to the .csv file\n",
    "    metrics: A dictionary of the metrics to request from the Perspective API (example: {'TOXICITY': {}, 'SEVERE_TOXICITY': {}})\n",
    "\"\"\"\n",
    "def format_eval_perspective(path, metrics, labels_present=True, limit=None):\n",
    "    df = pd.read_csv(path, encoding='latin-1')\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        if limit and index >= limit:\n",
    "            break\n",
    "        tweet_text = row['Text']\n",
    "        inputs.append({\n",
    "            'comment': { 'text': tweet_text },\n",
    "            'requestedAttributes': metrics\n",
    "        })\n",
    "        if labels_present:\n",
    "            labels.append(row['Label'])\n",
    "        else:\n",
    "            labels.append(\"None\")\n",
    "    return {\n",
    "        'Inputs': inputs,\n",
    "        'Labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are on a social media platform's moderation team. Your task is to decide whether the following tweet contains extremist content or not.\n",
      "Extremist content includes messages that are considered to be propoganda, radicalization, or recruitment. Please label the tweet as 'Propoganda', 'Radicalization', 'Recruitment', or 'None'.\n",
      "Your output must be in JSON format and with the following structure:\n",
      "{\"Label\": \"Propoganda\" or \"Radicalization\" or \"Recruitment\" or \"None\"}\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are on a social media platform's moderation team. \"\n",
    "system_message += \"Your task is to decide whether the following tweet contains extremist content or not.\\n\"\n",
    "system_message += \"Extremist content includes messages that are considered to be propoganda, radicalization, or recruitment. \"\n",
    "system_message += \"Please label the tweet as 'Propoganda', 'Radicalization', 'Recruitment', or 'None'.\\n\"\n",
    "system_message += \"Your output must be in JSON format and with the following structure:\\n{\\\"Label\\\": \\\"Propoganda\\\" or \\\"Radicalization\\\" or \\\"Recruitment\\\" or \\\"None\\\"}\"\n",
    "\n",
    "print(system_message)\n",
    "\n",
    "ISIS_dict_openai = format_eval_openai('Seed_MIWS/Seed_Dataset/ISIS_Seed_Complete.csv', system_message)\n",
    "WS_dict_openai = format_eval_openai('Seed_MIWS/Seed_Dataset/WS_Seed_Complete.csv', system_message)\n",
    "# tweets_dict_openai = format_eval_openai(\"tweets.csv\", system_message, labels_present=False, limit=1000)\n",
    "\n",
    "metrics = {'IDENTITY_ATTACK': {}, 'SEVERE_TOXICITY': {}, 'THREAT': {}}\n",
    "ISIS_dict_perspective = format_eval_perspective('Seed_MIWS/Seed_Dataset/ISIS_Seed_Complete.csv', metrics)\n",
    "WS_dict_perspective = format_eval_perspective('Seed_MIWS/Seed_Dataset/WS_Seed_Complete.csv', metrics)\n",
    "# tweets_dict_perspective = format_eval_openai(\"tweets.csv\", metrics, labels_present=False, limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict_openai = {\n",
    "    'Inputs': ISIS_dict_openai['Inputs'] + WS_dict_openai['Inputs'],\n",
    "    'Labels': ISIS_dict_openai['Labels'] + WS_dict_openai['Labels']\n",
    "}\n",
    "\n",
    "combined_dict_perspective = {\n",
    "    'Inputs': ISIS_dict_perspective['Inputs'] + WS_dict_perspective['Inputs'],\n",
    "    'Labels': ISIS_dict_perspective['Labels'] + WS_dict_perspective['Labels']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined openAI dictionary:\n",
      "Inputs shape: 398\n",
      "Labels shape: 398\n",
      "\n",
      "Combined Perspective dictionary:\n",
      "Inputs shape: 398\n",
      "Labels shape: 398\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined openAI dictionary:\")\n",
    "print(f\"Inputs shape: {len(combined_dict_openai['Inputs'])}\")\n",
    "print(f\"Labels shape: {len(combined_dict_openai['Labels'])}\")\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "print(\"Combined Perspective dictionary:\")\n",
    "print(f\"Inputs shape: {len(combined_dict_perspective['Inputs'])}\")\n",
    "print(f\"Labels shape: {len(combined_dict_perspective['Labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the combined dictionary to a new .json file at the specified path\n",
    "import json\n",
    "json_file_openai = json.dumps(combined_dict_openai, indent=4)\n",
    "json_file_perspective = json.dumps(combined_dict_perspective, indent=4)\n",
    "\n",
    "with open(\"eval_data_openai.json\", \"w\") as file:\n",
    "    file.write(json_file_openai)\n",
    "\n",
    "with open(\"eval_data_perspective.json\", \"w\") as file:\n",
    "    file.write(json_file_perspective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are on a social media platform\\'s moderation team. Your task is to decide whether the following tweet contains extremist content or not.\\nExtremist content includes messages that are considered to be propoganda, radicalization, or recruitment. Please label the tweet as \\'Propoganda\\', \\'Radicalization\\', \\'Recruitment\\', or \\'None\\'.\\nYour output must be in JSON format and with the following structure:\\n{\"Label\": \"Propoganda\" or \"Radicalization\" or \"Recruitment\" or \"None\"}'}, {'role': 'user', 'content': 'Coalition planes massacred these children in airstrikes\\nOn #Hit ,#Anbar\\nhttp://t.co/yCsEgkwDY6#Iraq'}]\n"
     ]
    }
   ],
   "source": [
    "print(combined_dict_openai['Inputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comment': {'text': 'Coalition planes massacred these children in airstrikes\\nOn #Hit ,#Anbar\\nhttp://t.co/yCsEgkwDY6#Iraq'}, 'requestedAttributes': {'IDENTITY_ATTACK': {}, 'SEVERE_TOXICITY': {}, 'THREAT': {}}}\n"
     ]
    }
   ],
   "source": [
    "print(combined_dict_perspective['Inputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are on a social media platform's moderation team. Your task is to decide whether the tweets provided to you contain extremist content or not. Content warning - many of these tweets will have content that can be offensive.\n",
      "Extremist content includes messages that are considered to be propoganda, radicalization, or recruitment. Please label each tweet as 'Propoganda', 'Radicalization', 'Recruitment', or 'None'.\n",
      "The format of the messages supplied to you will be as follows: \"Please label the following tweet as 'Propoganda', 'Radicalization', 'Recruitment', or 'None': [TWEET], with id of ID\", where TWEET will be replaced by each tweet's text content, and ID is the tweet's id.\n",
      "For each tweet, your output must be in JSON format and with the following structure:\n",
      "{\"ID\": ID, \"Label\": \"Propoganda\" or \"Radicalization\" or \"Recruitment\" or \"None\"}, where ID is the tweet's ID that was provided to you in the message.\n"
     ]
    }
   ],
   "source": [
    "gemini_sys_message = '''You are on a social media platform's moderation team. Your task is to decide whether the tweets provided to you contain extremist content or not.\n",
    "Extremist content includes messages that are considered to be propoganda, radicalization, or recruitment. Please label each tweet as 'Propoganda', 'Radicalization', 'Recruitment', or 'None'.\n",
    "The format of the messages supplied to you will be as follows: \"Please label the following tweet as 'Propoganda', 'Radicalization', 'Recruitment', or 'None': [TWEET], with id of ID\", where TWEET will be replaced by each tweet's text content, and ID is the tweet's id.\n",
    "For each tweet, your output must be in JSON format and with the following structure:\n",
    "{\"ID\": ID, \"Label\": \"Propoganda\" or \"Radicalization\" or \"Recruitment\" or \"None\"}, where ID is the tweet's ID that was provided to you in the message.'''\n",
    "print(gemini_sys_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "with open(\"../DiscordBot/tokens.json\", 'r') as file:\n",
    "    tokens = json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD version - this will error out due to hitting safety filters. See further below for working Gemini implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!gcloud auth application-default login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot get the response text.\nCannot get the Candidate text.\nResponse candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\nContent:\n{}\nCandidate:\n{\n  \"finish_reason\": \"SAFETY\",\n  \"safety_ratings\": [\n    {\n      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.45190093,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.4285921\n    },\n    {\n      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n      \"probability\": \"HIGH\",\n      \"blocked\": true,\n      \"probability_score\": 0.89823216,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.6970936\n    },\n    {\n      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n      \"probability\": \"LOW\",\n      \"probability_score\": 0.5125859,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.4723792\n    },\n    {\n      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.21223201,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severity_score\": 0.21502088\n    }\n  ]\n}\nResponse:\n{\n  \"candidates\": [\n    {\n      \"finish_reason\": \"SAFETY\",\n      \"safety_ratings\": [\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\",\n          \"probability_score\": 0.45190093,\n          \"severity\": \"HARM_SEVERITY_MEDIUM\",\n          \"severity_score\": 0.4285921\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"HIGH\",\n          \"blocked\": true,\n          \"probability_score\": 0.89823216,\n          \"severity\": \"HARM_SEVERITY_MEDIUM\",\n          \"severity_score\": 0.6970936\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"LOW\",\n          \"probability_score\": 0.5125859,\n          \"severity\": \"HARM_SEVERITY_MEDIUM\",\n          \"severity_score\": 0.4723792\n        },\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\",\n          \"probability_score\": 0.21223201,\n          \"severity\": \"HARM_SEVERITY_LOW\",\n          \"severity_score\": 0.21502088\n        }\n      ]\n    }\n  ],\n  \"usage_metadata\": {\n    \"prompt_token_count\": 123,\n    \"candidates_token_count\": 1,\n    \"total_token_count\": 124\n  }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:1746\u001b[0m, in \u001b[0;36mCandidate.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1746\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;66;03m# Enrich the error message with the whole Candidate.\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# The Content object does not have full information.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:1826\u001b[0m, in \u001b[0;36mContent.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts:\n\u001b[0;32m-> 1826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1827\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse candidate content has no parts (and thus no text).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1828\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The candidate is likely blocked by the safety filters.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1829\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1830\u001b[0m         \u001b[38;5;241m+\u001b[39m _dict_to_pretty_string(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[1;32m   1831\u001b[0m     )\n\u001b[1;32m   1832\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparts[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[0;31mValueError\u001b[0m: Response candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\nContent:\n{}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:1667\u001b[0m, in \u001b[0;36mGenerationResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;66;03m# Enrich the error message with the whole Response.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;66;03m# The Candidate object does not have full information.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:1750\u001b[0m, in \u001b[0;36mCandidate.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;66;03m# Enrich the error message with the whole Candidate.\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# The Content object does not have full information.\u001b[39;00m\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1751\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get the Candidate text.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1752\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1753\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCandidate:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1754\u001b[0m         \u001b[38;5;241m+\u001b[39m _dict_to_pretty_string(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[1;32m   1755\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot get the Candidate text.\nResponse candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\nContent:\n{}\nCandidate:\n{\n  \"finish_reason\": \"SAFETY\",\n  \"safety_ratings\": [\n    {\n      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.45190093,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.4285921\n    },\n    {\n      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n      \"probability\": \"HIGH\",\n      \"blocked\": true,\n      \"probability_score\": 0.89823216,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.6970936\n    },\n    {\n      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n      \"probability\": \"LOW\",\n      \"probability_score\": 0.5125859,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.4723792\n    },\n    {\n      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.21223201,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severity_score\": 0.21502088\n    }\n  ]\n}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 30\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text_response)\n\u001b[1;32m     22\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    I need you to review the following content to determine if it contains harmful language. The content should not be repeated or generated. Please analyze the text and provide feedback on whether it includes any of the following categories of extremist language: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPropoganda\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRadicalization\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecruitment\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m     24\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124mPlease respond with a summary indicating whether any extremist language is present and categorize it accordingly.\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_chat_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[43], line 19\u001b[0m, in \u001b[0;36mget_chat_response\u001b[0;34m(chat, prompt)\u001b[0m\n\u001b[1;32m     17\u001b[0m responses \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39msend_message(prompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m responses:\n\u001b[0;32m---> 19\u001b[0m     text_response\u001b[38;5;241m.\u001b[39mappend(\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text_response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/vertexai/generative_models/_generative_models.py:1671\u001b[0m, in \u001b[0;36mGenerationResponse.text\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcandidates[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1669\u001b[0m     \u001b[38;5;66;03m# Enrich the error message with the whole Response.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m     \u001b[38;5;66;03m# The Candidate object does not have full information.\u001b[39;00m\n\u001b[0;32m-> 1671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1672\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot get the response text.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1673\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1675\u001b[0m         \u001b[38;5;241m+\u001b[39m _dict_to_pretty_string(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[1;32m   1676\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot get the response text.\nCannot get the Candidate text.\nResponse candidate content has no parts (and thus no text). The candidate is likely blocked by the safety filters.\nContent:\n{}\nCandidate:\n{\n  \"finish_reason\": \"SAFETY\",\n  \"safety_ratings\": [\n    {\n      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.45190093,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.4285921\n    },\n    {\n      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n      \"probability\": \"HIGH\",\n      \"blocked\": true,\n      \"probability_score\": 0.89823216,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.6970936\n    },\n    {\n      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n      \"probability\": \"LOW\",\n      \"probability_score\": 0.5125859,\n      \"severity\": \"HARM_SEVERITY_MEDIUM\",\n      \"severity_score\": 0.4723792\n    },\n    {\n      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n      \"probability\": \"NEGLIGIBLE\",\n      \"probability_score\": 0.21223201,\n      \"severity\": \"HARM_SEVERITY_LOW\",\n      \"severity_score\": 0.21502088\n    }\n  ]\n}\nResponse:\n{\n  \"candidates\": [\n    {\n      \"finish_reason\": \"SAFETY\",\n      \"safety_ratings\": [\n        {\n          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n          \"probability\": \"NEGLIGIBLE\",\n          \"probability_score\": 0.45190093,\n          \"severity\": \"HARM_SEVERITY_MEDIUM\",\n          \"severity_score\": 0.4285921\n        },\n        {\n          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n          \"probability\": \"HIGH\",\n          \"blocked\": true,\n          \"probability_score\": 0.89823216,\n          \"severity\": \"HARM_SEVERITY_MEDIUM\",\n          \"severity_score\": 0.6970936\n        },\n        {\n          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n          \"probability\": \"LOW\",\n          \"probability_score\": 0.5125859,\n          \"severity\": \"HARM_SEVERITY_MEDIUM\",\n          \"severity_score\": 0.4723792\n        },\n        {\n          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n          \"probability\": \"NEGLIGIBLE\",\n          \"probability_score\": 0.21223201,\n          \"severity\": \"HARM_SEVERITY_LOW\",\n          \"severity_score\": 0.21502088\n        }\n      ]\n    }\n  ],\n  \"usage_metadata\": {\n    \"prompt_token_count\": 123,\n    \"candidates_token_count\": 1,\n    \"total_token_count\": 124\n  }\n}"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, ChatSession\n",
    "\n",
    "project_id = tokens['gcp_project_id']\n",
    "\n",
    "vertexai.init(project=project_id, location=\"us-central1\")\n",
    "\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-flash\"\n",
    ")\n",
    "\n",
    "chat = model.start_chat(response_validation=False)\n",
    "\n",
    "def get_chat_response(chat: ChatSession, prompt: str) -> str:\n",
    "    text_response = []\n",
    "    responses = chat.send_message(prompt, stream=True)\n",
    "    for chunk in responses:\n",
    "        text_response.append(chunk.text)\n",
    "    return \"\".join(text_response)\n",
    "\n",
    "prompt = '''\n",
    "    I need you to review the following content to determine if it contains harmful language. The content should not be repeated or generated. Please analyze the text and provide feedback on whether it includes any of the following categories of extremist language: 'Propoganda', 'Radicalization', 'Recruitment', or 'None'.\n",
    "\n",
    "Content to review:\n",
    "[Coalition planes massacred these children in airstrikes\\nOn #Hit ,#Anbar\\nhttp://t.co/yCsEgkwDY6#Iraq]\n",
    "\n",
    "Please respond with a summary indicating whether any extremist language is present and categorize it accordingly.\n",
    "'''\n",
    "print(get_chat_response(chat, prompt))\n",
    "\n",
    "# prompt = \"What are all the colors in a rainbow?\"\n",
    "# print(get_chat_response(chat, prompt))\n",
    "\n",
    "# prompt = \"Why does it appear when it rains?\"\n",
    "# print(get_chat_response(chat, prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKING version \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = '''You are on a social media platform's moderation team. I need you to review the following content as if it were a tweet to determine if it contains harmful and/or extrimist language. The content should not be repeated or generated. Please analyze the text and provide feedback on whether it includes any of the following categories of extremist language, and choose the one that fits best: 'Propaganda', 'Radicalization', 'Recruitment', or 'None'.\n",
    "\n",
    "Content to review:\n",
    "[Coalition planes massacred these children in airstrikes\\nOn #Hit ,#Anbar\\nhttp://t.co/yCsEgkwDY6#Iraq]\n",
    "\n",
    "Please respond with a summary indicating whether any extremist language is present and categorize it in only one of the provided categories accordingly.\n",
    "Please give your response as valid JSON in the following format: {\"Label\": <INSERT YOUR CATEGORIZATION HERE (\"Propaganda\" or \"Radicalization\" or \"Recruitment\" or \"None\")>, \"Reason\": <INSERT YOUR REASON HERE>}.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "genai.configure(api_key=tokens[\"gemini_google_ai_studio\"])\n",
    "model = genai.GenerativeModel(model_name='gemini-1.5-flash')\n",
    "def review_prompt(prompt):\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        safety_settings={\n",
    "            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE\n",
    "        }\n",
    "    )\n",
    "    try:\n",
    "        res = response.text\n",
    "        res_json = json.loads(response.text[7:][:-5])\n",
    "        return res_json\n",
    "    except:\n",
    "        # If the response doesn't contain text, check if the prompt was blocked.\n",
    "        print(response.prompt_feedback)\n",
    "        if len(response.candidates) > 0:\n",
    "            # Also check the finish reason to see if the response was blocked.\n",
    "            print(response.candidates[0].finish_reason)\n",
    "            # If the finish reason was SAFETY, the safety ratings have more details.\n",
    "            print(response.candidates[0].safety_ratings)\n",
    "        return {\"error\": str(response.prompt_feedback)}\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_reason: OTHER\n",
      "\n",
      "{'error': 'block_reason: OTHER\\n'}\n"
     ]
    }
   ],
   "source": [
    "res = review_prompt(combined_dict_gemini['Inputs'][173]['content'])\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_eval_gemini(path, system_message, labels_present=True, limit=None):\n",
    "    df = pd.read_csv(path, encoding='latin-1')\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for index, row in df.iterrows():\n",
    "        if limit and index >= limit:\n",
    "            break\n",
    "        tweet_text = row['Text']\n",
    "        prompt = \"You are on a social media platform's moderation team. I need you to review the following content as if it were a tweet to determine if it contains harmful and/or extrimist language. The content should not be repeated or generated. Please analyze the text and provide feedback on whether it includes any of the following categories of extremist language, and choose the one that fits best: 'Propaganda', 'Radicalization', 'Recruitment', or 'None'.\\n\\n\"\n",
    "        prompt += \"Content to review:\\n\"\n",
    "        prompt += f\"[{tweet_text}]\\n\\n\"\n",
    "        prompt += \"Please respond with a summary indicating whether any extremist language is present and categorize it in only one of the provided categories accordingly.\\n\"\n",
    "        prompt += \"Please give your response as valid JSON in the following format: {\\\"Label\\\": <INSERT YOUR CATEGORIZATION HERE (\\\"Propaganda\\\" or \\\"Radicalization\\\" or \\\"Recruitment\\\" or \\\"None\\\")>, \\\"Reason\\\": <INSERT YOUR REASON HERE>}.\"\n",
    "        inputs.append(\n",
    "            {\n",
    "                'content': prompt\n",
    "            }\n",
    "        )\n",
    "        if labels_present:\n",
    "            labels.append(row['Label'])\n",
    "        else:\n",
    "            labels.append(\"None\")\n",
    "    return {\n",
    "        'Inputs': inputs,\n",
    "        'Labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISIS_dict_gemini = format_eval_gemini('Seed_MIWS/Seed_Dataset/ISIS_Seed_Complete.csv', metrics)\n",
    "WS_dict_gemini = format_eval_gemini('Seed_MIWS/Seed_Dataset/WS_Seed_Complete.csv', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dict_gemini = {\n",
    "    'Inputs': ISIS_dict_gemini['Inputs'] + WS_dict_gemini['Inputs'],\n",
    "    'Labels': ISIS_dict_gemini['Labels'] + WS_dict_gemini['Labels']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Gemini dictionary:\n",
      "Inputs shape: 398\n",
      "Labels shape: 398\n"
     ]
    }
   ],
   "source": [
    "print(\"Combined Gemini dictionary:\")\n",
    "print(f\"Inputs shape: {len(combined_dict_gemini['Inputs'])}\")\n",
    "print(f\"Labels shape: {len(combined_dict_gemini['Labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_gemini = json.dumps(combined_dict_gemini, indent=4)\n",
    "\n",
    "with open(\"eval_data_gemini.json\", \"w\") as file:\n",
    "    file.write(json_file_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0 out of 398\n",
      "finished 1 out of 398\n",
      "finished 2 out of 398\n",
      "finished 3 out of 398\n",
      "finished 4 out of 398\n",
      "finished 5 out of 398\n",
      "finished 6 out of 398\n",
      "finished 7 out of 398\n",
      "finished 8 out of 398\n",
      "finished 9 out of 398\n",
      "finished 10 out of 398\n",
      "finished 11 out of 398\n",
      "finished 12 out of 398\n",
      "finished 13 out of 398\n",
      "finished 14 out of 398\n",
      "finished 15 out of 398\n",
      "finished 16 out of 398\n",
      "finished 17 out of 398\n",
      "finished 18 out of 398\n",
      "finished 19 out of 398\n",
      "finished 20 out of 398\n",
      "finished 21 out of 398\n",
      "finished 22 out of 398\n",
      "finished 23 out of 398\n",
      "finished 24 out of 398\n",
      "finished 25 out of 398\n",
      "finished 26 out of 398\n",
      "finished 27 out of 398\n",
      "finished 28 out of 398\n",
      "finished 29 out of 398\n",
      "finished 30 out of 398\n",
      "finished 31 out of 398\n",
      "finished 32 out of 398\n",
      "finished 33 out of 398\n",
      "finished 34 out of 398\n",
      "finished 35 out of 398\n",
      "finished 36 out of 398\n",
      "finished 37 out of 398\n",
      "finished 38 out of 398\n",
      "finished 39 out of 398\n",
      "finished 40 out of 398\n",
      "finished 41 out of 398\n",
      "finished 42 out of 398\n",
      "finished 43 out of 398\n",
      "finished 44 out of 398\n",
      "finished 45 out of 398\n",
      "finished 46 out of 398\n",
      "finished 47 out of 398\n",
      "finished 48 out of 398\n",
      "finished 49 out of 398\n",
      "finished 50 out of 398\n",
      "finished 51 out of 398\n",
      "finished 52 out of 398\n",
      "finished 53 out of 398\n",
      "finished 54 out of 398\n",
      "finished 55 out of 398\n",
      "finished 56 out of 398\n",
      "finished 57 out of 398\n",
      "finished 58 out of 398\n",
      "finished 59 out of 398\n",
      "finished 60 out of 398\n",
      "finished 61 out of 398\n",
      "finished 62 out of 398\n",
      "finished 63 out of 398\n",
      "finished 64 out of 398\n",
      "finished 65 out of 398\n",
      "finished 66 out of 398\n",
      "finished 67 out of 398\n",
      "finished 68 out of 398\n",
      "finished 69 out of 398\n",
      "finished 70 out of 398\n",
      "finished 71 out of 398\n",
      "finished 72 out of 398\n",
      "finished 73 out of 398\n",
      "finished 74 out of 398\n",
      "finished 75 out of 398\n",
      "finished 76 out of 398\n",
      "finished 77 out of 398\n",
      "finished 78 out of 398\n",
      "finished 79 out of 398\n",
      "finished 80 out of 398\n",
      "finished 81 out of 398\n",
      "finished 82 out of 398\n",
      "finished 83 out of 398\n",
      "finished 84 out of 398\n",
      "finished 85 out of 398\n",
      "finished 86 out of 398\n",
      "finished 87 out of 398\n",
      "finished 88 out of 398\n",
      "finished 89 out of 398\n",
      "finished 90 out of 398\n",
      "finished 91 out of 398\n",
      "finished 92 out of 398\n",
      "finished 93 out of 398\n",
      "finished 94 out of 398\n",
      "finished 95 out of 398\n",
      "finished 96 out of 398\n",
      "finished 97 out of 398\n",
      "finished 98 out of 398\n",
      "finished 99 out of 398\n",
      "finished 100 out of 398\n",
      "finished 101 out of 398\n",
      "finished 102 out of 398\n",
      "finished 103 out of 398\n",
      "finished 104 out of 398\n",
      "finished 105 out of 398\n",
      "finished 106 out of 398\n",
      "finished 107 out of 398\n",
      "finished 108 out of 398\n",
      "finished 109 out of 398\n",
      "finished 110 out of 398\n",
      "finished 111 out of 398\n",
      "finished 112 out of 398\n",
      "finished 113 out of 398\n",
      "finished 114 out of 398\n",
      "finished 115 out of 398\n",
      "finished 116 out of 398\n",
      "finished 117 out of 398\n",
      "finished 118 out of 398\n",
      "finished 119 out of 398\n",
      "finished 120 out of 398\n",
      "finished 121 out of 398\n",
      "finished 122 out of 398\n",
      "finished 123 out of 398\n",
      "finished 124 out of 398\n",
      "finished 125 out of 398\n",
      "finished 126 out of 398\n",
      "finished 127 out of 398\n",
      "finished 128 out of 398\n",
      "finished 129 out of 398\n",
      "finished 130 out of 398\n",
      "finished 131 out of 398\n",
      "finished 132 out of 398\n",
      "finished 133 out of 398\n",
      "finished 134 out of 398\n",
      "finished 135 out of 398\n",
      "finished 136 out of 398\n",
      "finished 137 out of 398\n",
      "finished 138 out of 398\n",
      "finished 139 out of 398\n",
      "finished 140 out of 398\n",
      "finished 141 out of 398\n",
      "finished 142 out of 398\n",
      "finished 143 out of 398\n",
      "finished 144 out of 398\n",
      "finished 145 out of 398\n",
      "finished 146 out of 398\n",
      "finished 147 out of 398\n",
      "finished 148 out of 398\n",
      "finished 149 out of 398\n",
      "finished 150 out of 398\n",
      "finished 151 out of 398\n",
      "finished 152 out of 398\n",
      "finished 153 out of 398\n",
      "finished 154 out of 398\n",
      "finished 155 out of 398\n",
      "finished 156 out of 398\n",
      "finished 157 out of 398\n",
      "finished 158 out of 398\n",
      "finished 159 out of 398\n",
      "finished 160 out of 398\n",
      "finished 161 out of 398\n",
      "finished 162 out of 398\n",
      "finished 163 out of 398\n",
      "finished 164 out of 398\n",
      "finished 165 out of 398\n",
      "finished 166 out of 398\n",
      "finished 167 out of 398\n",
      "finished 168 out of 398\n",
      "finished 169 out of 398\n",
      "finished 170 out of 398\n",
      "finished 171 out of 398\n",
      "finished 172 out of 398\n",
      "block_reason: OTHER\n",
      "\n",
      "finished 173 out of 398\n",
      "finished 174 out of 398\n",
      "finished 175 out of 398\n",
      "finished 176 out of 398\n",
      "finished 177 out of 398\n",
      "finished 178 out of 398\n",
      "finished 179 out of 398\n",
      "finished 180 out of 398\n",
      "finished 181 out of 398\n",
      "finished 182 out of 398\n",
      "finished 183 out of 398\n",
      "finished 184 out of 398\n",
      "finished 185 out of 398\n",
      "finished 186 out of 398\n",
      "finished 187 out of 398\n",
      "finished 188 out of 398\n",
      "finished 189 out of 398\n",
      "finished 190 out of 398\n",
      "finished 191 out of 398\n",
      "finished 192 out of 398\n",
      "finished 193 out of 398\n",
      "finished 194 out of 398\n",
      "finished 195 out of 398\n",
      "finished 196 out of 398\n",
      "finished 197 out of 398\n",
      "finished 198 out of 398\n",
      "finished 199 out of 398\n",
      "finished 200 out of 398\n",
      "finished 201 out of 398\n",
      "finished 202 out of 398\n",
      "finished 203 out of 398\n",
      "finished 204 out of 398\n",
      "finished 205 out of 398\n",
      "finished 206 out of 398\n",
      "finished 207 out of 398\n",
      "finished 208 out of 398\n",
      "finished 209 out of 398\n",
      "finished 210 out of 398\n",
      "finished 211 out of 398\n",
      "finished 212 out of 398\n",
      "finished 213 out of 398\n",
      "finished 214 out of 398\n",
      "finished 215 out of 398\n",
      "finished 216 out of 398\n",
      "finished 217 out of 398\n",
      "finished 218 out of 398\n",
      "finished 219 out of 398\n",
      "finished 220 out of 398\n",
      "finished 221 out of 398\n",
      "finished 222 out of 398\n",
      "finished 223 out of 398\n",
      "finished 224 out of 398\n",
      "finished 225 out of 398\n",
      "finished 226 out of 398\n",
      "finished 227 out of 398\n",
      "finished 228 out of 398\n",
      "finished 229 out of 398\n",
      "finished 230 out of 398\n",
      "finished 231 out of 398\n",
      "finished 232 out of 398\n",
      "finished 233 out of 398\n",
      "finished 234 out of 398\n",
      "finished 235 out of 398\n",
      "finished 236 out of 398\n",
      "finished 237 out of 398\n",
      "finished 238 out of 398\n",
      "finished 239 out of 398\n",
      "finished 240 out of 398\n",
      "finished 241 out of 398\n",
      "finished 242 out of 398\n",
      "finished 243 out of 398\n",
      "finished 244 out of 398\n",
      "finished 245 out of 398\n",
      "finished 246 out of 398\n",
      "finished 247 out of 398\n",
      "finished 248 out of 398\n",
      "finished 249 out of 398\n",
      "finished 250 out of 398\n",
      "finished 251 out of 398\n",
      "finished 252 out of 398\n",
      "finished 253 out of 398\n",
      "finished 254 out of 398\n",
      "finished 255 out of 398\n",
      "finished 256 out of 398\n",
      "finished 257 out of 398\n",
      "finished 258 out of 398\n",
      "finished 259 out of 398\n",
      "finished 260 out of 398\n",
      "finished 261 out of 398\n",
      "finished 262 out of 398\n",
      "finished 263 out of 398\n",
      "finished 264 out of 398\n",
      "finished 265 out of 398\n",
      "finished 266 out of 398\n",
      "finished 267 out of 398\n",
      "finished 268 out of 398\n",
      "finished 269 out of 398\n",
      "finished 270 out of 398\n",
      "finished 271 out of 398\n",
      "finished 272 out of 398\n",
      "finished 273 out of 398\n",
      "finished 274 out of 398\n",
      "finished 275 out of 398\n",
      "finished 276 out of 398\n",
      "finished 277 out of 398\n",
      "finished 278 out of 398\n",
      "finished 279 out of 398\n",
      "finished 280 out of 398\n",
      "finished 281 out of 398\n",
      "finished 282 out of 398\n",
      "finished 283 out of 398\n",
      "finished 284 out of 398\n",
      "finished 285 out of 398\n",
      "finished 286 out of 398\n",
      "finished 287 out of 398\n",
      "finished 288 out of 398\n",
      "finished 289 out of 398\n",
      "finished 290 out of 398\n",
      "finished 291 out of 398\n",
      "finished 292 out of 398\n",
      "finished 293 out of 398\n",
      "finished 294 out of 398\n",
      "finished 295 out of 398\n",
      "finished 296 out of 398\n",
      "finished 297 out of 398\n",
      "finished 298 out of 398\n",
      "finished 299 out of 398\n",
      "finished 300 out of 398\n",
      "finished 301 out of 398\n",
      "finished 302 out of 398\n",
      "finished 303 out of 398\n",
      "finished 304 out of 398\n",
      "finished 305 out of 398\n",
      "finished 306 out of 398\n",
      "finished 307 out of 398\n",
      "finished 308 out of 398\n",
      "finished 309 out of 398\n",
      "finished 310 out of 398\n",
      "finished 311 out of 398\n",
      "finished 312 out of 398\n",
      "finished 313 out of 398\n",
      "finished 314 out of 398\n",
      "finished 315 out of 398\n",
      "finished 316 out of 398\n",
      "finished 317 out of 398\n",
      "finished 318 out of 398\n",
      "finished 319 out of 398\n",
      "finished 320 out of 398\n",
      "finished 321 out of 398\n",
      "finished 322 out of 398\n",
      "finished 323 out of 398\n",
      "finished 324 out of 398\n",
      "finished 325 out of 398\n",
      "finished 326 out of 398\n",
      "finished 327 out of 398\n",
      "finished 328 out of 398\n",
      "finished 329 out of 398\n",
      "finished 330 out of 398\n",
      "finished 331 out of 398\n",
      "finished 332 out of 398\n",
      "finished 333 out of 398\n",
      "finished 334 out of 398\n",
      "finished 335 out of 398\n",
      "finished 336 out of 398\n",
      "finished 337 out of 398\n",
      "finished 338 out of 398\n",
      "finished 339 out of 398\n",
      "finished 340 out of 398\n",
      "finished 341 out of 398\n",
      "finished 342 out of 398\n",
      "finished 343 out of 398\n",
      "finished 344 out of 398\n",
      "finished 345 out of 398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 346 out of 398\n",
      "finished 347 out of 398\n",
      "finished 348 out of 398\n",
      "finished 349 out of 398\n",
      "finished 350 out of 398\n",
      "finished 351 out of 398\n",
      "finished 352 out of 398\n",
      "finished 353 out of 398\n",
      "finished 354 out of 398\n",
      "finished 355 out of 398\n",
      "finished 356 out of 398\n",
      "finished 357 out of 398\n",
      "finished 358 out of 398\n",
      "finished 359 out of 398\n",
      "finished 360 out of 398\n",
      "finished 361 out of 398\n",
      "finished 362 out of 398\n",
      "finished 363 out of 398\n",
      "finished 364 out of 398\n",
      "finished 365 out of 398\n",
      "finished 366 out of 398\n",
      "finished 367 out of 398\n",
      "finished 368 out of 398\n",
      "finished 369 out of 398\n",
      "finished 370 out of 398\n",
      "finished 371 out of 398\n",
      "finished 372 out of 398\n",
      "finished 373 out of 398\n",
      "finished 374 out of 398\n",
      "finished 375 out of 398\n",
      "finished 376 out of 398\n",
      "finished 377 out of 398\n",
      "finished 378 out of 398\n",
      "finished 379 out of 398\n",
      "finished 380 out of 398\n",
      "finished 381 out of 398\n",
      "finished 382 out of 398\n",
      "finished 383 out of 398\n",
      "finished 384 out of 398\n",
      "finished 385 out of 398\n",
      "finished 386 out of 398\n",
      "finished 387 out of 398\n",
      "finished 388 out of 398\n",
      "finished 389 out of 398\n",
      "finished 390 out of 398\n",
      "finished 391 out of 398\n",
      "finished 392 out of 398\n",
      "finished 393 out of 398\n",
      "finished 394 out of 398\n",
      "finished 395 out of 398\n",
      "finished 396 out of 398\n",
      "finished 397 out of 398\n"
     ]
    }
   ],
   "source": [
    "predicted = []\n",
    "for i in range(len(combined_dict_gemini['Inputs'])):\n",
    "    predicted.append(review_prompt(combined_dict_gemini['Inputs'][i]['content']))\n",
    "    print(f\"finished {i} out of {len(combined_dict_gemini['Inputs'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n"
     ]
    }
   ],
   "source": [
    "print(len(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_predictions_gemini = json.dumps({\"predictions\": predicted}, indent=4)\n",
    "\n",
    "with open(\"predictions_gemini.json\", \"w\") as file:\n",
    "    file.write(json_file_predictions_gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
